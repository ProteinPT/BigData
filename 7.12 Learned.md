# Python으로 배우는 외부데이터 수집

## 웹크롤링/스크래핑

> 1. 웹 크롤링/ 스크래핑
> 2. 스크래핑
> 3. 데이터 저장
> 4. 스크래핑 프로세스

### 웹 페이지 추출

- 추출 시 HTTP 헤더와 HTML의 meta태그를 기반으로 인코딩 방식을 판별 필요
- urllib.request 모듈을 사용하여 추출
- urlopen()함수에  URL지정하면 추출 가능
- HTTP 헤더 변경 불가,  Basic인증을 위해 복잡한 처리 필요 
- 헤더변경 및 인증을 위해 Request 모듈 사용 필요

#### 크롤링/스크래핑

- 크롤링: 하이퍼링크를 순회하며 `웹페이지를 다운`받는 것

- 스크래핑: 다운받은 웹페이지로부터 `정보를 추출`하는 것



```python
f= urlopen('http://hanbit.co.kr')
# 변수에 HTTPResponse 객체 저장되어 있음

f.getheader('Content-Type')
> 'text/html; charset=UTF-8'
# 요청하고 response로 받고, 해당 헤더에 html형태로 저장되어 있음
```



### 문자코드 다루기

- HTTP 헤더를 참조해서 적절한 인코딩 방식으로 디코딩 필요
- HTTP 응답의 Content-Type 헤더를 참조하면 해당 페이지의 인코딩 방식 확인 가능
- `HTTPResponse.read()` 메소드로 추출할 수 있는 본문은  bytes 자료형
- HTTPMessage 객체의 `get_content_charset()` 메소드를 사용하여 인코딩 추출 가능



### HTTP 헤더에서 인코딩 방식 추출

```python
f= urlopen('http://hanbit.co.kr')
encoding= f.info().get_content_charset(failobj="utf-8") 
# HTTP 헤더를 기반으로 인코딩 방식을 추출(명시돼 있지 않을 경우 utf-8을 사용)
text= f.read().decode(encoding)
# 추출한 인코딩 방식으로 디코딩

```



### 웹페이지에서 데이터 추출

-  정규 표현식을 이용한 스크래핑
- XML 파서를 이용한 스크래핑
- HTML 스크래핑



#### 1. 정규 표현식 스크래핑

> `특정 검색 패턴`에 대한 하나 이상의 일치 항목을 검색하고 검색된 텍스트로부터 정보를 추출하는데 매우 유용하게 사용 가능한 표현식

- 정규식 패턴(파이썬 정규식 예제 검색을 통해 깊이있는 이해와 어떻게 활용하면 좋을 지 스스로 탐구해볼 것!)

  | 구분    | 표현       | 설명                                      |
  | ------- | ---------- | ----------------------------------------- |
  | Anchors | ^The       | The로 시작하는 모든 문자열을 매칭         |
  |         | end$       | end로 끝나는 문자열과 매칭                |
  |         | ^The end $ | The end와 정확하게 일치하는 문자열을 매칭 |
  |         | roar       | roar가 들어있는 모든 문자열과 매칭        |

  | 구분        | 표현       | 설명                                                  |
  | ----------- | ---------- | ----------------------------------------------------- |
  | Quantifiers | abc*       | ab그리고 0개 이상의 c를 포함한 문자열과 매칭          |
  |             | abc+       | ab그리고 1개 이상의 c를 포함한 문자열과 매칭          |
  |             | abc?       | ab 그리고 0개 또는 1개의 c를 포함한 문자열과 매칭     |
  |             | abc{2}     | ab그리고 2개의 c를 포함한 문자열과 매칭               |
  |             | abc{2,}    | ab그리고 2개의 c를 포함한 문자열과 매칭               |
  |             | abc{2,5}   | ab그리고 2개 이상 5개 이하의 c를 포함한 문자열과 매칭 |
  |             | a(bc)*     | a그리고 0개 이상의 bc를 포함한 문자열과 매칭          |
  |             | a(bc){2,5} | a그리고 2개이상 5개 이하의 bc를 포함한 문자열과 매칭  |

  | 구분        | 표현    | 설명                                   |
  | ----------- | ------- | -------------------------------------- |
  | OR operator | a(b\|c) | a그리고 b또는 c를 포함한 문자열과 매칭 |
  |             | a[bc]   | a그리고 b또는 c를 포함한 문자열과 매칭 |

  | 구분                | 표현      | 설명                                                     |
  | ------------------- | --------- | -------------------------------------------------------- |
  | Bracket expressions | [abc]     | a또는 b또는 c를 포함하는 문자열과 매칭, a\|b\|c 와 동일  |
  |                     | [a-c]     | a또는 b또는 c를 포함하는 문자열과 매칭                   |
  |                     | [0-9]%    | 0이상 9 이하의 숫자 그리고 % 문자를 포함한 문자열과 매칭 |
  |                     | [^a-zA-Z] | 영문이 아닌 문자와 매칭. ^는 부정표현으로 사용           |

  | 구분              | 표현 | 설명                              |
  | ----------------- | ---- | --------------------------------- |
  | Character classes | \d   | 모든 숫자와 일치, [0-9]와 동일    |
  |                   | \D   | \d와 반대.[ ^0-9]와 동일          |
  |                   | \n   | 줄바꿈 문자                       |
  |                   | \r   | 캐리지 리턴                       |
  |                   | \s   | 공백                              |
  |                   | \t   | 탭 문자                           |
  |                   | \w   | 영숫자 문자나 언더바 [a-zA-X0-9_] |
  |                   | \W   | \w와 반대 [ ^a-zA-X0-9_]          |

  .        > 모든 문자중 하나

  a(bc) > 소괄호는 캡쳐그룹을 생성

---

### XML 을 이용한 스크래핑

>  RSS(Really  Simple Syndication, Rich Site Summary)
>
> 뉴스나 블로그 등 업데이트가 빈번한 사이트에서 주로 사용하는 콘텐츠 표현 방식
>
> 구독자들에게 업데이트된 정보를 용이하게 제공하기 위해 XML 기반으로 정보 표현 및 제공



- 컴퓨터가 문자열, 구문을 읽기 위해서 구조화된 방법으로 메모리에 올려 놓는 과정을 수행하는 프로그램을 `parcer`라고 한다. 
- XML을 컴퓨터가 읽고 분석할 수 있도록 `.parse('rss.xml')` 함수를 사용한다.

- 부모 자식 노드가 있는 것 처럼  XML, HTML 도 이러한 구조(상위 하위 디렉토리)로 이뤄어져 있다.
- 최상위= `Root  디렉토리`, 하위 디렉토리= `서브 디렉토리`
- XML 구조에서 최상위의 위치를 획득하기 위해 ` getroot()` 함수를 사용한다.
- `Pandas` =2차원 데이터 구조[행렬(데이터프레임)를 잘 읽고 쓰기 용이하게 해주는 파이썬 라이브러리



---

### CSV 형식으로 저장

> CSV(Comma-Seperated Values)?
>
> 하나의 레코드를 한 라인에 저장하는 텍스트 포맷 파일
>
> 각 라인의 컬럼값은 쉼표(콤마) 등의 구분자 사용

```python
import csv
with open('rich_city.csv','b',newlien='',encoding='utf-8') as f:
# 첫 번째 매개변수는 파일 객체, 두 번째 매개변수는 필드명 리스트 지정
writer=csv.DictWriter(f,['rank','city','population'])
#JSON 포멧 형태로 저장 .DictWriter, 컬럼명 부여
writer.writeheader()
#첫 번째 줄에 헤더 입력
writer.writerows([
    {'rank': 1, 'city': '상파울루', 'population': 241500000}
    ---
    ---
    ---
    ---
])
#여러개의 데이터를 딕셔너리 형태로 작성
import chardet
char_dic=chardet.detect(open('rich_city.csv','rb').read())
char_dic['encoding']
```





---

### JSON 형식으로 저장

> Key-value 형식의 데이터 객체를 저장 전달하기 위해 텍스트 형식의 개방형 표준 포멧
>
> 플랫폼 및 프로그래밍 언어 독립적 데이터 포멧

#####  < JSON 데이터 타입>

- 문자열
- 숫자
- 객체
- 배열
- 불리언
- null

#####  시그니처 기호가 `대괄호`면 `리스트`

- 파일시스템 read, write 할때 쓰는 함수=  `open()`

- with 읽거나 쓰고나서 반드시 close 해줘야 하는데 자동적으로 close 할 수 있도록 `with 구문` 을 쓴다

  > ```python
  > with open('rich_cities.json','w') as fw
  > json.dump(cities, fw)
  > #파일 시스템 저장하기위 한 JSON 라이브러리 함수 .dump()
  > json_file= json.load(fr)
  > #저장된 파일 불러오기 .load()
  > ```



---

### SQLite3 DBMS 로 저장

> SQLite3 DBMS?
>
> 파일 시스템 기반의 경량 관계형 DBMS
>
> 경량관계형 DBMS로 스마트폰 등의 embedded 환경에서 널리 사용

12분 쯤 남았을 때 코드 설명





---

## 파이썬 스크래핑 프로세스

#### 1. 웹 페이지 크롤링

- ##### fetch(url)

- 매개변수로 지정한 URL 웹페이지 추출

#### 2. 스크래핑

- ##### scrape(html)

- 매개변수로  html을 받고, 정규 표현식을 사용해 HTML에서 도서정보 추출

#### 3. 데이터 저장

- ##### save(db_path,books)

- 매개변수로 books라는 도서목록을 받고, db_path로 지정된 SQLiteDB에 저장



### HTML 스크래핑

- #### lxml

- #### Beautiful Soup

13분남았을 때 강의 다시 보기

- 스크래핑을 위한 라이브러리는 모두` XPath와  CSS selector`를 이용하여 스크래핑 수행



#### - lxml을 이용한 스크래핑

- lxml.etree: ElementTRee를 확장한 XML 파서
- lxml.html: xml.etree 기반 invaild HTML도 다룰 수 있게 해주는 HTML 파서



#### - Beautiful Soup를 이용한 스크래핑



## URL 기초 지식

#### URL 구조

##### http://example.com/main/index?q=python#lead

- http: `스키마`(+https)
- example.com> `authority` 도메인명: 포트번호
- main/index > `path`:  호스트 내부에서의 리소스 경로
- q=python > `query`:  ? 뒤에 나오는 경로와 다른 방법으로 리소스를 표현하는 방법
- lead > `flagment`: # 뒤에 나오는 리소스 내부의 특정부분



### 절대 URL 상대 URL

> 절대 URL: 프로토콜(http,https), //로 시작하는 URL
>
> 상대 URL: 절대 URL 기준으로 상대적인 경로를 잡는 URL, /로 시작하는URL

##### 상대 -> 절대 URL 변환

- urljoin()함수

  ```python
  from urllib.parse import urljoin
  base_url='http://yes.com/books/top/html'
  urljoin(base_url,'//cdn.yes.com/logo.png')
  
  >'http://cdn.yes.com/logo.png
  
  urljoin(base_url,'/articles/')
  > 'http://yes.com/articles/'
  ```

  

### 퍼머링크와 링크 구조 패턴

> 퍼머링크?
>
> 영구(Permanent)와 링크(Link)의 조합

12분 남았을 때 부터 듣기0

